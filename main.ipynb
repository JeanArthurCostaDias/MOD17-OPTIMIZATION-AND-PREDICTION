{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa4d2a66",
      "metadata": {},
      "source": [
        "#### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd067dd-9e45-48f0-a0da-64b16008ba7e",
      "metadata": {
        "id": "8dd067dd-9e45-48f0-a0da-64b16008ba7e"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from time import time\n",
        "from contextlib import contextmanager\n",
        "from typing import List, Union\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr,kruskal\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
        "from sklearn.model_selection import train_test_split,TimeSeriesSplit\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "\n",
        "from minisom import MiniSom\n",
        "\n",
        "from tsai.all import *\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "from fastai.vision.all import *\n",
        "from fastai.text.all import *\n",
        "from fastai.collab import *\n",
        "from fastai.tabular.all import *\n",
        "\n",
        "import torch_optimizer\n",
        "\n",
        "os.environ[\"DEVICE\"] = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e842775-2d5e-421e-bd60-19894e5a4e5a",
      "metadata": {
        "id": "2e842775-2d5e-421e-bd60-19894e5a4e5a"
      },
      "source": [
        "## Pré-processamento dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2bc501-61b1-4bfa-8b57-6f43009ee34c",
      "metadata": {
        "id": "bf2bc501-61b1-4bfa-8b57-6f43009ee34c"
      },
      "source": [
        "### Cálculo do GPP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c5af31-6272-45c2-8d2f-5d3535f66aad",
      "metadata": {
        "id": "e0c5af31-6272-45c2-8d2f-5d3535f66aad"
      },
      "outputs": [],
      "source": [
        "@contextmanager\n",
        "def cwd(path: str) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Context manager para mudar o diretório de trabalho.\n",
        "    Mantém o diretório original após a execução do bloco de código.\n",
        "    \"\"\"\n",
        "\n",
        "    oldpwd = os.getcwd()\n",
        "    os.chdir(path)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        os.chdir(oldpwd)\n",
        "\n",
        "def constructor_2(path):\n",
        "    df = pd.read_csv(path, parse_dates=['date'])\n",
        "    # Definindo a coluna 'system:time_start' como índice\n",
        "    hora_inicio = df['date'].min().floor('d')\n",
        "    df.index = hora_inicio + pd.to_timedelta(df.index, unit='h')\n",
        "    # Removendo a coluna 'system:time_start'\n",
        "    df = df.drop(['date','.geo'], axis=1)\n",
        "    df = df.drop('system:index',axis=1)\n",
        "    return df\n",
        "\n",
        "def modis_date(data_str):\n",
        "    ano = int(data_str[1:5])\n",
        "    dia_ano = int(data_str[5:])\n",
        "    data = datetime.strptime('{}-{}'.format(ano, dia_ano), '%Y-%j')\n",
        "    return data\n",
        "\n",
        "def df_xy(x,y):\n",
        "    x_df = pd.DataFrame(x[:,0]).reset_index().drop('index',axis=1)\n",
        "    x_df.columns = [f'janela {i}' for i in range(len(x_df.columns))]\n",
        "\n",
        "    y_df = pd.DataFrame(y).reset_index().drop('index',axis=1)\n",
        "    y_df.columns = [f'previsao {i}' for i in range(len(y_df.columns))]\n",
        "\n",
        "    return pd.concat([x_df,y_df],axis=1)\n",
        "\n",
        "@contextmanager\n",
        "def cwd(path: str) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Context manager para mudar o diretório de trabalho.\n",
        "    Mantém o diretório original após a execução do bloco de código.\n",
        "    \"\"\"\n",
        "\n",
        "    oldpwd = os.getcwd()\n",
        "    os.chdir(path)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        os.chdir(oldpwd)\n",
        "\n",
        "def read_data():\n",
        "    with cwd('dados/Merra/dados_otimizados'):\n",
        "        dados = []\n",
        "        for csv in sorted(os.listdir()):\n",
        "            name = csv.split('_')[1]\n",
        "            df = pd.read_csv(csv)\n",
        "            datetime = df['index']\n",
        "            df.index = pd.to_datetime(datetime)\n",
        "            df = df['GPP']\n",
        "            smoothed_csv = ExponentialSmoothing(df, initialization_method=\"heuristic\",freq='D').fit(optimized=True).fittedvalues\n",
        "            dados.append(pd.DataFrame(smoothed_csv,columns=[name],index=datetime))\n",
        "        return dados\n",
        "\n",
        "gpp_cax, gpp_peru, gpp_santarem = read_data()\n",
        "gpp_todos = pd.concat([gpp_peru,gpp_santarem,gpp_cax],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3de17b",
      "metadata": {},
      "source": [
        "### Divisão do dataset em treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8840a571",
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = train_test_split(gpp_todos,test_size=0.15,shuffle=False,random_state=1) # Não embaralhado\n",
        "train_date = train.index\n",
        "test_date = test.index\n",
        "\n",
        "gpp_peru_train, gpp_santarem_train, gpp_cax_train = gpp_peru.loc[train_date].copy(),gpp_santarem.loc[train_date].copy(),gpp_cax.loc[train_date].copy()\n",
        "gpp_peru_test, gpp_santarem_test, gpp_cax_test = gpp_peru.loc[test_date].copy(),gpp_santarem.loc[test_date].copy(),gpp_cax.loc[test_date].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0290b2c3-581f-4d22-904b-9b739a3e3b3e",
      "metadata": {
        "id": "0290b2c3-581f-4d22-904b-9b739a3e3b3e"
      },
      "source": [
        "### Criação da janela temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "669de903",
      "metadata": {},
      "outputs": [],
      "source": [
        "peru_x_test,peru_y_test = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_peru_test)                                                                                     \n",
        "santarem_x_test,santarem_y_test = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_santarem_test)\n",
        "cax_x_test,cax_y_test = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_cax_test)\n",
        "\n",
        "\n",
        "# Transformação em Dataframe para melhor manipulação dos dados\n",
        "peru_xtest = pd.DataFrame(peru_x_test.reshape(-1, 8))\n",
        "peru_ytest = pd.DataFrame(peru_y_test)\n",
        "peru_ytest['localidade'] = 'peru'\n",
        "\n",
        "santarem_xtest = pd.DataFrame(santarem_x_test.reshape(-1, 8))\n",
        "santarem_ytest = pd.DataFrame(santarem_y_test)\n",
        "santarem_ytest['localidade'] = 'santarem'\n",
        "\n",
        "cax_xtest = pd.DataFrame(cax_x_test.reshape(-1, 8))\n",
        "cax_ytest = pd.DataFrame(cax_y_test)\n",
        "cax_ytest['localidade'] = 'caxiuana'\n",
        "\n",
        "x_test_df = pd.concat([peru_xtest,\n",
        "                       santarem_xtest,\n",
        "                       cax_xtest,\n",
        "                       ],\n",
        "                      ignore_index=True)\n",
        "\n",
        "\n",
        "y_test_df = pd.concat([pd.DataFrame(peru_ytest),\n",
        "                       pd.DataFrame(santarem_ytest),\n",
        "                       pd.DataFrame(cax_ytest),\n",
        "                       ],\n",
        "                      ignore_index=True)\n",
        "\n",
        "\n",
        "X_test = x_test_df.values.reshape(-1, 1, 8) # (n° observações, características, janela de observação)\n",
        "y_test = y_test_df.drop('localidade',axis=1).values\n",
        "y_labels = y_test_df['localidade']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cb0d4b-62ad-4889-b4bc-9907da3d46c0",
      "metadata": {
        "id": "14cb0d4b-62ad-4889-b4bc-9907da3d46c0"
      },
      "outputs": [],
      "source": [
        "peru_x_train,peru_y_train = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_peru_train) \n",
        "santarem_x_train,santarem_y_train = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_santarem_train)\n",
        "cax_x_train,cax_y_train = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_cax_train)\n",
        "\n",
        "peru_df = df_xy(peru_x_train,peru_y_train)\n",
        "santarem_df = df_xy(santarem_x_train,santarem_y_train)\n",
        "cax_df = df_xy(cax_x_train,cax_y_train)\n",
        "\n",
        "peru_df['localidade'] = 'peru'\n",
        "santarem_df['localidade'] = 'santarem'\n",
        "cax_df['localidade'] = 'caxiuana'\n",
        "\n",
        "\n",
        "gpp_geral = pd.concat([peru_df,santarem_df,cax_df]).reset_index().drop('index',axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c79b917-c462-4fe1-9abd-b21626a9d19b",
      "metadata": {
        "id": "1c79b917-c462-4fe1-9abd-b21626a9d19b"
      },
      "source": [
        "### Geração de Características das séries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e9c178c",
      "metadata": {},
      "outputs": [],
      "source": [
        "previsao = [f'previsao {i}' for i in range(0,8)]\n",
        "\n",
        "X = gpp_geral.drop(columns=['localidade']).drop(columns=previsao)  # Aqui ficam 8 colunas representado instantes da janela temporal\n",
        "y = gpp_geral[previsao] # Aqui fica os próximos 8 valores da sequência\n",
        "z = gpp_geral[['localidade']]\n",
        "\n",
        "ts_features_df = get_ts_features(X=X.values.reshape(X.shape[0],1,X.shape[1]),y=y.values,features='efficient')\n",
        "ts_features_df = ts_features_df.dropna(axis=1) # Removendo colunas com valores nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce44e040",
      "metadata": {},
      "source": [
        "### Clusterização com SOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5cc339-183b-4cc5-89ca-c7de36115e80",
      "metadata": {
        "id": "ec5cc339-183b-4cc5-89ca-c7de36115e80"
      },
      "outputs": [],
      "source": [
        "norm = preprocessing.Normalizer()\n",
        "\n",
        "datos = norm.fit_transform(ts_features_df.values)\n",
        "\n",
        "map_size = round(np.sqrt(round(5 * np.sqrt(len(ts_features_df))))) # arredondando pra um inteiro com raiz exata = 324 = 18x18\n",
        "\n",
        "som_shape = (map_size, map_size)\n",
        "som = MiniSom(som_shape[0], som_shape[1], datos.shape[1],neighborhood_function='gaussian',activation_distance='euclidean',random_seed=1)\n",
        "\n",
        "som.train_batch(datos, 100, verbose=False)\n",
        "print(som.quantization_error(datos))\n",
        "\n",
        "winner_coordinates = np.array([som.winner(x) for x in datos]).T\n",
        "\n",
        "cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)\n",
        "gpp_geral['cluster'] = np.array(cluster_index)\n",
        "\n",
        "# Contar o número de ocorrências de cada cluster por localidade\n",
        "cluster_counts = gpp_geral.groupby(['localidade', 'cluster']).size()\n",
        "\n",
        "# Filtrar os clusters para apenas os que possuem mais de 6 ocorrências para remover outliers\n",
        "valid_clusters = cluster_counts[cluster_counts > 6].index\n",
        "\n",
        "# Manter apenas as linhas cujo cluster e localidade estão nos clusters e localidades válidos\n",
        "gpp_geral = gpp_geral[gpp_geral.set_index(['localidade', 'cluster']).index.isin(valid_clusters)].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78685742",
      "metadata": {},
      "source": [
        "### Divisão Final do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9bb200",
      "metadata": {},
      "outputs": [],
      "source": [
        "previsao = [f'previsao {i}' for i in range(0,8)]\n",
        "X = gpp_geral.drop(columns=['localidade', 'cluster']).drop(columns=previsao) \n",
        "y = gpp_geral[previsao]\n",
        "z = gpp_geral[['localidade','cluster']]\n",
        "\n",
        "# Dividir os dados em treino e validação estratificados por 'localidade' e 'cluster'\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.177, stratify=z,random_state=1)\n",
        "\n",
        "train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
        "X_train = train_data.drop(columns=previsao)\n",
        "y_train = train_data[previsao]\n",
        "\n",
        "val_data = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)\n",
        "X_val = val_data.drop(columns=previsao)\n",
        "y_val = val_data[previsao]\n",
        "\n",
        "X_train_np = np.array(X_train)\n",
        "X_val_np = np.array(X_val)\n",
        "X_test_np = np.array(X_test)\n",
        "y_train_np = np.array(y_train)\n",
        "y_val_np = np.array(y_val)\n",
        "y_test_np = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9313f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y, splits = combine_split_data(xs=[X_train_np, X_val_np, X_test_np], ys=[y_train_np, y_val_np, y_test_np])\n",
        "plot_splits(splits)\n",
        "\n",
        "tfms = [None, TSForecasting()]\n",
        "get_splits_len(splits) # [1872, 401, 408] ~= 70%,15%,15%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874d9fea",
      "metadata": {},
      "source": [
        "### Testes com os Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9338814e",
      "metadata": {},
      "outputs": [],
      "source": [
        "archs = [\n",
        "         (XCMPlus, {}),\n",
        "         (ConvTranPlus, {}),\n",
        "         (TSSequencerPlus, {}),      # Arquiteturas que estou testando (ainda arbitrário).\n",
        "         (RNNPlus, {}),                         #Os dicionários do lado do nome de cada arquitetura são específicos dos parâmetros\n",
        "         (ResNetPlus, {}),                              # EX: (LSTM, {'n_layers':1, 'bidirectional': False})\n",
        "         (InceptionTimePlus, {}),\n",
        "         (TSTPlus, {}),\n",
        "         (TransformerLSTMPlus, {}),\n",
        "         (XceptionTimePlus, {}),\n",
        "         (TransformerGRUPlus, {}),\n",
        "         (PatchTST, {}),\n",
        "\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9285d53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def test_archs(epochs):\n",
        "#     results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'mae_valid','rmse_valid','mae_test','rmse_test','time'])\n",
        "#     i=0\n",
        "#     for _, (arch, k) in enumerate(archs):\n",
        "#         print(arch.__name__)\n",
        "#         learn = TSForecaster(X, y, splits=splits, path='models', tfms=tfms, batch_tfms=TSStandardize(), arch=arch, metrics=[mae,rmse],device=default_device(),loss_func=HuberLoss('mean'))\n",
        "#         lr = learn.lr_find() # learning rate find\n",
        "#         start = time.time()\n",
        "#         learn.fit_one_cycle(epochs, lr_max=lr.valley)\n",
        "#         elapsed = time.time() - start\n",
        "#         vals = learn.recorder.values[-1]\n",
        "#         raw_preds, target, _ = learn.get_X_preds(X[splits[2]], y[splits[2]])\n",
        "#         mae_test = mean_absolute_error(raw_preds.flatten(),target.flatten())\n",
        "#         mse_test = mean_squared_error(raw_preds.flatten(),target.flatten())\n",
        "#         rmse_test = np.sqrt(mse_test)\n",
        "#         results.loc[i] = [arch.__name__, k, count_parameters(learn.model), vals[0], vals[1], vals[2],vals[3],mae_test,rmse_test, int(elapsed)]\n",
        "#         results.sort_values(by=['mae_valid'], ascending=False, kind='stable', ignore_index=True, inplace=True)\n",
        "#         clear_output()\n",
        "#         display(results)\n",
        "#         i+=1\n",
        "#     results.to_csv(f'resultados_{epochs}_epocas.csv')\n",
        "\n",
        "# for epocas in range(50,350,50):\n",
        "#     test_archs(epocas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e426fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_ConvTranPlus(trial):\n",
        "    # Categorical parameter\n",
        "    #arch_name = trial.suggest_categorical('arch',list(architectures.keys()))\n",
        "\n",
        "    fc_dropout = trial.suggest_float('fc_dropout', 0.1, 0.8)\n",
        "\n",
        "    encoder_dropout = trial.suggest_float('window_perc', 0.1, 0.8)\n",
        "\n",
        "    arch_config = {\n",
        "        'encoder_dropout': encoder_dropout,\n",
        "        'fc_dropout': fc_dropout,\n",
        "    }\n",
        "    \n",
        "    #learning_rate_model = trial.suggest_float(\"learning_rate_model\", 1e-5, 1e-2, log=True)  # search through all float values between 0.0 and 0.5 in log increment steps\n",
        "    Huber_delta = trial.suggest_float(\"Huber_delta\", 1, 2)\n",
        "    \n",
        "    standardize_sample = trial.suggest_categorical('by_sample', [True, False])\n",
        "    standardize_var = trial.suggest_categorical('by_var', [True, False])\n",
        "\n",
        "    arch = ConvTranPlus\n",
        "\n",
        "    learn = TSForecaster(X, y, splits=splits, path='models', tfms=tfms,\n",
        "                         batch_tfms=TSStandardize(by_sample=standardize_sample, by_var=standardize_var),arch=arch,\n",
        "                         arch_config= arch_config,\n",
        "                         #cbs=[ShowGraph(),PredictionDynamics(alpha=.5, size=75)],\n",
        "                         loss_func=HuberLoss('mean',Huber_delta),seed=1)\n",
        "    lr = learn.lr_find() # learning rate find\n",
        "    with ContextManagers([learn.no_logging(),learn.no_bar()]):\n",
        "        for epoch in range(50):\n",
        "            learn.fit_one_cycle(1, lr_max=lr.valley)\n",
        "            intermediate_value = learn.recorder.values[-1][1]\n",
        "            trial.report(intermediate_value, epoch)\n",
        "            # Check if trial should be pruned\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "    with open(\"./optuna_tests/ConvTranPlus/{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        pickle.dump(learn, fout)\n",
        "    return intermediate_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d8d2eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_Xception(trial):\n",
        "    # Categorical parameter\n",
        "    #arch_name = trial.suggest_categorical('arch',list(architectures.keys()))\n",
        "\n",
        "    nf = trial.suggest_int('nf', 16, 384)  # Ajustado para o intervalo original de 16 a 384\n",
        "    nb_filters = trial.suggest_categorical('nb_filters', [None, 16, 32, 64, 128, 256])  # Opções para nb_filters\n",
        "\n",
        "    arch_config = {\n",
        "        'nf': nf,\n",
        "        'nb_filters': nb_filters,\n",
        "    }\n",
        "\n",
        "    #learning_rate_model = trial.suggest_float(\"learning_rate_model\", 1e-5, 1e-2, log=True)  # search through all float values between 0.0 and 0.5 in log increment steps\n",
        "    Huber_delta = trial.suggest_float(\"Huber_delta\", 1, 2)\n",
        "    \n",
        "    standardize_sample = trial.suggest_categorical('by_sample', [True, False])\n",
        "    standardize_var = trial.suggest_categorical('by_var', [True, False])\n",
        "\n",
        "    arch = XceptionTimePlus\n",
        "\n",
        "    learn = TSForecaster(X, y, splits=splits, path='models', tfms=tfms,\n",
        "                         batch_tfms=TSStandardize(by_sample=standardize_sample, by_var=standardize_var),arch=arch,\n",
        "                         arch_config= arch_config,\n",
        "                         #cbs=[ShowGraph(),PredictionDynamics(alpha=.5, size=75)],\n",
        "                         loss_func=HuberLoss('mean',Huber_delta),seed=1)\n",
        "    \n",
        "    with ContextManagers([learn.no_logging(),learn.no_bar()]):\n",
        "        lr = learn.lr_find() # learning rate find\n",
        "        for epoch in range(50):\n",
        "            learn.fit_one_cycle(1, lr_max=lr.valley)\n",
        "            intermediate_value = learn.recorder.values[-1][1]\n",
        "            trial.report(intermediate_value, epoch)\n",
        "            # Check if trial should be pruned\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "    with open(\"./optuna_tests/XceptionPlus/{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        pickle.dump(learn, fout)\n",
        "    return intermediate_value\n",
        "\n",
        "study_conv = run_optuna_study(objective_ConvTranPlus,sampler= optuna.samplers.TPESampler(n_startup_trials=200),\n",
        "                          pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource=50, reduction_factor=3, bootstrap_count=5),\n",
        "                          n_trials=1000,gc_after_trial=True,direction=\"minimize\",show_plots=False)\n",
        "\n",
        "\n",
        "study_Xception = run_optuna_study(objective_Xception,sampler= optuna.samplers.TPESampler(n_startup_trials=200),\n",
        "                          pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource=50, reduction_factor=3, bootstrap_count=5),\n",
        "                          n_trials=1000,gc_after_trial=True,direction=\"minimize\",show_plots=False)\n",
        "\n",
        "print(f\"O Melhor modelo foi o de número {study_conv.best_trial.number}\")\n",
        "print(f\"\"\" Acesse a pasta optuna_tests/ConvTranPlus/{study_conv.best_trial.number}.pickle e coloque o modelo no github \"\"\")\n",
        "\n",
        "print(f\"O Melhor modelo foi o de número {study_Xception.best_trial.number}\")\n",
        "print(f\"\"\" Acesse a pasta optuna_tests/XceptionPlus/{study_Xception.best_trial.number}.pickle e coloque o modelo no github \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3359df6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"{}.pickle\".format(study.best_trial.number), \"rb\") as fin:\n",
        "#     learner = pickle.load(fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b889aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# raw_preds, target, preds = learner.get_X_preds(X[splits[2]], y[splits[2]])\n",
        "# mae_test = mean_absolute_error(raw_preds,target)\n",
        "# mse_test = mean_squared_error(raw_preds,target)\n",
        "# rmse_test = np.sqrt(mse_test)\n",
        "# r2_test = r2_score(y_pred=raw_preds,y_true=target)\n",
        "# print(r2_test)\n",
        "# print(mae_test)\n",
        "# print(rmse_test)\n",
        "# print(np.corrcoef(raw_preds.flatten(), target.flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c4159b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# preds_df = pd.concat([pd.DataFrame(raw_preds),y_labels.to_frame()],axis=1)\n",
        "# target_df = pd.concat([pd.DataFrame(y_test),y_labels.to_frame()],axis=1)\n",
        "\n",
        "# dfs_preds = {}\n",
        "# dfs_target = {}\n",
        "\n",
        "# for localidade in preds_df['localidade'].unique():\n",
        "#     dfs_preds[localidade] = preds_df[preds_df['localidade'] == localidade]\n",
        "\n",
        "# for localidade in target_df['localidade'].unique():\n",
        "#     dfs_target[localidade] = target_df[target_df['localidade'] == localidade]\n",
        "\n",
        "# # Acessando os DataFrames separados\n",
        "\n",
        "# df_peru_pred = dfs_preds['peru'].drop('localidade',axis=1)\n",
        "# df_santarem_pred = dfs_preds['santarem'].drop('localidade',axis=1)\n",
        "# df_caxiuana_pred = dfs_preds['caxiuana'].drop('localidade',axis=1)\n",
        "\n",
        "# df_peru_target = dfs_target['peru'].drop('localidade',axis=1)\n",
        "# df_santarem_target = dfs_target['santarem'].drop('localidade',axis=1)\n",
        "# df_caxiuana_target = dfs_target['caxiuana'].drop('localidade',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee4229c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_summary(df_pred,df_target):\n",
        "#     results = {}\n",
        "#     for i in dfs_preds:\n",
        "#         mae_test = mean_absolute_error(y_pred=df_pred[i].drop('localidade',axis=1),y_true=df_target[i].drop('localidade',axis=1))\n",
        "#         mse_test = mean_squared_error(y_pred=df_pred[i].drop('localidade',axis=1),y_true=df_target[i].drop('localidade',axis=1))\n",
        "#         rmse_test = np.sqrt(mse_test)\n",
        "#         r2_test = r2_score(y_pred=df_pred[i].drop('localidade',axis=1),y_true=df_target[i].drop('localidade',axis=1))\n",
        "#         results[i] = [r2_test,mae_test,rmse_test,np.corrcoef(df_pred[i].drop('localidade',axis=1).values.flatten(), df_target[i].drop('localidade',axis=1).values.flatten())[0][1]]\n",
        "\n",
        "#         residuos = df_pred[i].drop('localidade',axis=1).values - df_target[i].drop('localidade',axis=1).values\n",
        "#         plt.figure(figsize=(30, 9))\n",
        "#         plt.subplot(2, 1, 1)\n",
        "#         plt.plot(df_pred[i].drop('localidade',axis=1).values.flatten(), label='Previsões', alpha=1)\n",
        "#         plt.plot(df_target[i].drop('localidade',axis=1).values.flatten(), label='Valores reais', alpha=0.7)\n",
        "#         plt.xlabel('Índice da observação')\n",
        "#         plt.ylabel('Valor')\n",
        "#         plt.title(f'Comparação entre Previsões e Valores Reais ({i})')\n",
        "#         plt.legend()\n",
        "#         plt.grid(True)\n",
        "        \n",
        "#         plt.subplot(2, 1, 2)\n",
        "#         plt.plot(residuos.flatten(), label='Resíduos', color='red')\n",
        "#         plt.xlabel('Índice da observação')\n",
        "#         plt.ylabel('Resíduo')\n",
        "#         plt.title(f'Resíduos {i} (Previsões - Valores reais)')\n",
        "#         plt.legend()\n",
        "#         plt.grid(True)\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#         residuos_flatten = residuos.flatten()\n",
        "\n",
        "#         # Criar figura e eixos para o subplot\n",
        "#         fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "#         # Plot do residual plot\n",
        "#         sns.scatterplot(x=df_pred[i].drop('localidade',axis=1).values.flatten(), y=residuos_flatten, ax=axes[0])\n",
        "#         axes[0].set_title('Residual Plot')\n",
        "#         axes[0].set_xlabel('Valores Previstos')\n",
        "#         axes[0].set_ylabel('Resíduos')\n",
        "\n",
        "#         # Plot do histograma dos resíduos\n",
        "#         sns.histplot(residuos_flatten, ax=axes[1])\n",
        "#         axes[1].set_title('Histograma dos Resíduos')\n",
        "#         axes[1].set_xlabel('Resíduos')\n",
        "#         axes[1].set_ylabel('Contagem')\n",
        "\n",
        "#         # Ajuste o layout\n",
        "#         plt.tight_layout()\n",
        "\n",
        "#         # Mostrar o subplot\n",
        "#         plt.show()\n",
        "\n",
        "#     return pd.DataFrame(results,index=['R²','MAE','RMSE','Corr'])\n",
        "\n",
        "# display(get_summary(dfs_preds,dfs_target))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
