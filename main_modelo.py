# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUMYFzOU6FbxH-oVAN5VcAjymA6Dz6Wf
"""
from datetime import datetime
from time import time
from contextlib import contextmanager
from typing import List, Union

import os
import torch
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr,kruskal
from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt

from minisom import MiniSom

from tsai.all import *

import optuna
from torch_optimizer import *


os.environ["DEVICE"] = "cuda"

"""## Pré-processamento dos dados

### Cálculo do GPP
"""

@contextmanager
def cwd(path: str) -> None:

    """
    Context manager para mudar o diretório de trabalho.
    Mantém o diretório original após a execução do bloco de código.
    """

    oldpwd = os.getcwd()
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(oldpwd)

def constructor_2(path):
    df = pd.read_csv(path, parse_dates=['date'])
    # Definindo a coluna 'system:time_start' como índice
    hora_inicio = df['date'].min().floor('d')
    df.index = hora_inicio + pd.to_timedelta(df.index, unit='h')
    # Removendo a coluna 'system:time_start'
    df = df.drop(['date','.geo'], axis=1)
    df = df.drop('system:index',axis=1)
    return df

def modis_date(data_str):
    ano = int(data_str[1:5])
    dia_ano = int(data_str[5:])
    data = datetime.strptime('{}-{}'.format(ano, dia_ano), '%Y-%j')
    return data

#==================================== Funções do Software do MOD17

def linear_constraint(
        xmin, xmax, form: str = None):
    assert form is None or form in ('reversed', 'binary'),\
        'Argument "form" must be None or one of: "reversed", "binary"'
    assert form == 'binary' or np.any(xmax >= xmin),\
        'xmax must be greater than/ equal to xmin'
    if form == 'reversed':
        return lambda x: np.where(x >= xmax, 0,
            np.where(x < xmin, 1, 1 - np.divide(
                np.subtract(x, xmin), xmax - xmin)))
    if form == 'binary':
        return lambda x: np.where(x == 1, xmax, xmin)
    return lambda x: np.where(x >= xmax, 1,
        np.where(x < xmin, 0,
            np.divide(np.subtract(x, xmin), xmax - xmin)))

def _gpp(params, fpar, tmin, vpd, par):
    'Daily GPP as static method, avoids overhead of class instantiation'
    # "params" argument should be a Sequence of atomic parameter values
    #   in the order prescribed by "required_parameters"
    tmin_scalar = linear_constraint(params[1], params[2])(tmin)
    vpd_scalar = linear_constraint(
        params[3], params[4], form = 'reversed')(vpd)
    lue = params[0] * tmin_scalar * vpd_scalar
    return 1e3 * lue * fpar * par

def _par(sw_rad: Number, period_hrs: Number = 1) -> Number:
    '''
    Calculates daily total photosynthetically active radiation (PAR) from
    (hourly) incoming short-wave radiation (SW_rad). PAR is assumed to
    be 45% of SW_rad.

    Parameters
    ----------
    swrad : int or float or numpy.ndarray
        Incoming short-wave radiation (W m-2)
    period_hrs : int
        Period over which radiation is measured, in hours (Default: 1)

    Returns
    -------
    int or float or numpy.ndarray
    '''
    # Convert SW_rad from [W m-2] to [MJ m-2], then take 45%;
    #   3600 secs hr-1 times (1 MJ / 1e6 Joules) == 0.0036
    return 0.45 * (0.0036 * (24 / period_hrs) * sw_rad)

def _vpd(qv10m: Number, pressure: Number, tmean: Number) -> Number:
    '''
    Computes vapor pressure deficit (VPD) from surface meteorology.

    Parameters
    ----------
    qv10m : int or float or numpy.ndarray
        Water vapor mixing ratio at 10-meter height (Pa)
    pressure : int or float or numpy.ndarray
        Atmospheric pressure (Pa)
    tmean : int or float or numpy.ndarray
        Mean daytime temperature (degrees C)

    Returns
    -------
    int or float or numpy.ndarray
    '''
    # Actual vapor pressure (Gates 1980, Biophysical Ecology, p.311)
    avp = (qv10m * pressure) / (0.622 + (0.379 * qv10m))
    # Saturation vapor pressure (similar to FAO formula)
    svp = 610.7 * np.exp((17.38 * tmean) / (239 + tmean))
    return svp - avp
#==================================== Funções do Software do MOD17



def create_drivers():
    """Função para calcular o valor dos parâmetros necessários para calcular o GPP de cada área"""
    date = 0
    drivers_dict = {}
    with cwd('./dados/Merra'):
        for folder in os.listdir():
            with cwd(folder):
                csvs = sorted(os.listdir())
                tday = constructor_2(f'{csvs[4]}') - 273.15 #( K° -> C°)
                tmin = tday['T10M'].resample('D').min()['2000-02-26':].values  # Reamostrando para Temperatura mínima diária
                tmean = tday['T10M'].resample('D').mean().values  # Temperatura média diária
                tmean_vpd = tday['T10M'].between_time('9:00', '21:00').resample('D').mean()['2000-02-26':].values

                # Leitura e pré-processamento dos dados de razão de mistura de vapor de água
                qv10m = constructor_2(f'{csvs[2]}')['2000-02-26':] # unidade: "Mass fraction" (kg/kg)
                qv10m = pd.to_numeric(qv10m['QV10M']).between_time('9:00', '21:00').resample('D').mean().values

                # Leitura e pré-processamento dos dados de pressão atmosférica
                ps = constructor_2(f'{csvs[1]}') # unidade: Pa  # Removendo vírgulas dos valores
                ps['PS'] = pd.to_numeric(ps['PS'])
                ps = ps.between_time('9:00', '21:00').resample('D').mean()['2000-02-26':]
                ps = pd.to_numeric(ps['PS']).values

                # Leitura e pré-processamento dos dados de radiação solar incidente
                SWGNT = constructor_2(f'{csvs[3]}')['SWGNT'].resample('D').mean()['2000-02-26':].values # unidade: W/m^2

                # Leitura e pré-processamento dos dados de Fração de Absorção de Luz Fotossinteticamente Ativa (FPAR)
                fpar = pd.read_csv(f'{csvs[0]}', header=None,na_values='F') # unidade: (%)
                fpar.index = fpar[2].apply(modis_date)  # Convertendo a data no formato MODIS

                # Arrumando o dataframe
                # ================================
                fpar = fpar.drop([0,1,2,3,4], axis=1)
                fpar.columns = range(len(fpar.columns))
                fpar = fpar[144]

                # ================================

                # Tratamento de dados faltando usando interpolação linear (8 dias -> diário)

                fpar = fpar['2000-02-26':'2022-01-01'].resample('D').interpolate('linear')[:-1]
                fpar = fpar.values if (folder != 'amazonia peruana') and (folder != 'guiana')  else fpar.values/100


                drivers = [
                    fpar[:][...,None],
                    tmin[:][...,None],
                    _vpd(qv10m, ps, tmean_vpd)[...,None],
                    _par(SWGNT)[:][...,None]
                ]
                drivers[2] = np.where(drivers[2] < 0, 0, drivers[2]) # Set negative VPD to zero


                date = tday['T10M'].resample('D').min()['2000-02-26':].index
                drivers_dict[folder] = drivers
    return date, drivers_dict


parametros = [0.001405, -8.0, 9.09, 1000.0, 4000.0, 26.9, 2.0, 2.0, 1.1, 0.162, 0.00604, 0.00519, 0.00397]

drivers = create_drivers()


index_date = drivers[0] # Data associada aos dados

drivers_peru, drivers_caxiuana, drivers_guiana, drivers_santarem = drivers[1].values() # Dados de cada região


gpp_peru = np.nanmean(_gpp(parametros,*drivers_peru), axis = -1) # gpp calculado

gpp_gui = np.nanmean(_gpp(parametros,*drivers_guiana), axis = -1) # gpp calculado

gpp_santarem = np.nanmean(_gpp(parametros,*drivers_santarem), axis = -1)

gpp_cax = np.nanmean(_gpp(parametros,*drivers_caxiuana), axis = -1)


gpp_peru = pd.DataFrame(gpp_peru,index=index_date)
gpp_gui = pd.DataFrame(gpp_gui,index=index_date)
gpp_santarem = pd.DataFrame(gpp_santarem,index=index_date)
gpp_cax = pd.DataFrame(gpp_cax,index=index_date)

gpp_peru = ExponentialSmoothing(gpp_peru, initialization_method="heuristic",freq='D').fit(optimized=True).fittedvalues
gpp_gui = ExponentialSmoothing(gpp_gui, initialization_method="heuristic",freq='D').fit(optimized=True).fittedvalues
gpp_santarem = ExponentialSmoothing(gpp_santarem, initialization_method="heuristic",freq='D').fit(optimized=True).fittedvalues
gpp_cax = ExponentialSmoothing(gpp_cax, initialization_method="heuristic",freq='D').fit(optimized=True).fittedvalues

gpp_gui = pd.DataFrame(gpp_gui,index=index_date) # Convertendo de volta pra um dataframe
gpp_peru = pd.DataFrame(gpp_peru,index=index_date) # Convertendo de volta pra um dataframe
gpp_santarem = pd.DataFrame(gpp_santarem,index=index_date)
gpp_cax = pd.DataFrame(gpp_cax,index=index_date)

"""### Criação da janela temporal"""

peru_x,peru_y = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_peru) #  stride = n datapoints the window is moved ahead along the sequence.
                                                                                    # Default: 1. If None, stride=window_length (no overlap)
santarem_x,santarem_y = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_santarem)
cax_x,cax_y = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_cax)
gui_x,gui_y = SlidingWindow(8,horizon=8,seq_first=True,stride = None)(gpp_gui)


def df_xy(x,y):
    x_df = pd.DataFrame(x[:,0]).reset_index().drop('index',axis=1)
    x_df.columns = [f'janela {i}' for i in range(len(x_df.columns))]

    y_df = pd.DataFrame(y).reset_index().drop('index',axis=1)
    y_df.columns = [f'previsao {i}' for i in range(len(y_df.columns))]

    return pd.concat([x_df,y_df],axis=1)

peru_df = df_xy(peru_x,peru_y)
santarem_df = df_xy(santarem_x,santarem_y)
cax_df = df_xy(cax_x,cax_y)
gui_df = df_xy(gui_x,gui_y)



peru_df['localidade'] = 'peru'
santarem_df['localidade'] = 'santarem'
cax_df['localidade'] = 'caxiuana'
gui_df['localidade'] = 'guiana'

#Código usável para 1 horizonte

# peru_x,peru_y = SlidingWindow(15,horizon=1,seq_first=True,stride = None)(gpp_peru) #  stride = n datapoints the window is moved ahead along the sequence. Default: 1. If None, stride=window_length (no overlap)
# santarem_x,santarem_y = SlidingWindow(15,horizon=1,seq_first=True,stride = None)(gpp_santarem)
# cax_x,cax_y = SlidingWindow(15,horizon=1,seq_first=True,stride = None)(gpp_cax)

# Dataframe de cada localidade
# peru_df = pd.DataFrame(peru_x[:,0],peru_y[:,0]).reset_index()
# peru_df.columns = range(len(peru_df.columns))

# santarem_df = pd.DataFrame(santarem_x[:,0],santarem_y[:,0]).reset_index()
# santarem_df.columns = range(len(santarem_df.columns))

# cax_df = pd.DataFrame(cax_x[:,0],cax_y[:,0]).reset_index()
# cax_df.columns = range(len(cax_df.columns))

# peru_df['localidade'] = 'peru'
# santarem_df['localidade'] = 'santarem'
# cax_df['localidade'] = 'caxiuana'
gpp_geral = pd.concat([peru_df,santarem_df,cax_df,gui_df]).reset_index().drop('index',axis=1)

"""### Clusterização com MiniSom"""

norm = preprocessing.Normalizer()

datos = norm.fit_transform(gpp_geral.drop('localidade',axis=1).values)

quant_error = {'som':[],'quant_error':[],'sigma':[],'learning_rate':[]}

map_size = round(5 * np.sqrt(len(gpp_geral))) # arredondando pra um inteiro com raiz exata = 324 = 18x18

for i in range(1,100):
    som_shape = (18, 18)
    lr = np.random.random()
    sigma = np.random.random()
    som = MiniSom(som_shape[0], som_shape[1], datos.shape[1], sigma=sigma, learning_rate=lr,
                  neighborhood_function='gaussian',activation_distance='euclidean')

    som.train_batch(datos, 1000, verbose=False)
    quant_error['som'].append(som)
    quant_error['quant_error'].append(som.quantization_error(datos))
    quant_error['sigma'].append(sigma)
    quant_error['learning_rate'].append(lr)

i = np.argmin(np.array(quant_error['quant_error']))
som = quant_error['som'][i]

# each neuron represents a cluster
winner_coordinates = np.array([som.winner(x) for x in datos]).T
# with np.ravel_multi_index we convert the bidimensional
# coordinates to a monodimensional index

cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)
gpp_geral['cluster'] = np.array(cluster_index)


# Contar o número de ocorrências de cada cluster por localidade
cluster_counts = gpp_geral.groupby(['localidade', 'cluster']).size()

# Filtrar os clusters para apenas os que possuem mais de 6 ocorrências
valid_clusters = cluster_counts[cluster_counts > 6].index

# Manter apenas as linhas cujo cluster e localidade estão nos clusters e localidades válidos
gpp_geral = gpp_geral[gpp_geral.set_index(['localidade', 'cluster']).index.isin(valid_clusters)].copy()

"""### Divisão de treino e Teste"""

df = gpp_geral.reset_index().drop('index',axis=1)

janela = [f'janela {i}' for i in range(0,8)]
previsao = [f'previsao {i}' for i in range(0,8)]


X = df.drop(columns=['localidade', 'cluster']).drop(columns=previsao)  # Aqui ficam 15 colunas representado instantes da janela temporal
y = df[previsao] # Aqui fica o 8° valor da sequência
z = df[['localidade','cluster']]

# Dividir os dados em treino e teste estratificados por 'localidade' e 'cluster'
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=z)

train_data = pd.concat([X_train, y_train], axis=1).sample(frac=1).reset_index(drop=True)
X_train = train_data.drop(columns=previsao)
y_train = train_data[previsao]

# Dividindo os dados em validação

X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.50, stratify=z.loc[X_test.index])

val_data = pd.concat([X_val, y_val], axis=1).sample(frac=1).reset_index(drop=True)
X_val = val_data.drop(columns=previsao)
y_val = val_data[previsao]

X_train_np = np.array(X_train)
X_val_np = np.array(X_val)
X_test_np = np.array(X_test)
y_train_np = np.array(y_train)
y_val_np = np.array(y_val)
y_test_np = np.array(y_test)



#plits = TSSplitter(valid_size=0.2, show_plot=True)(y_train)
X, y, splits = combine_split_data(xs=[X_train_np, X_val_np, X_test_np], ys=[y_train_np, y_val_np, y_test_np]) # estrutura pro modelo entender a divisão do dataset em treino teste e validação
plot_splits(splits)

bs = 512 # batch size

tfms = [None, TSForecasting()]
#dsets = TSDatasets(X, y.values, tfms=tfms, splits=splits)
#dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2], batch_tfms=TSStandardize()) # batch_tfms = batch transformations
### Testes com os modelos

archs = [
         (ConvTranPlus, {}),
         (TSSequencerPlus, {}),      # Arquiteturas que estou testando (ainda arbitrário).
         (RNNPlus, {}),                         #Os dicionários do lado do nome de cada arquitetura são específicos dos parâmetros
         (ResNetPlus, {}),                              # EX: (LSTM, {'n_layers':1, 'bidirectional': False})
         (InceptionTimePlus, {}),
         (TSTPlus, {}),
         (TransformerLSTMPlus, {}),
         (XceptionTimePlus, {}),
         (TransformerGRUPlus, {}),
         (XCMPlus, {}),

        ]

otimizadores = {
  'PID': PID,
  'Lamb': Lamb,
  'NovoGrad': NovoGrad,
  'QHAdam': QHAdam,
  'QHM': QHM,
  'RAdam': RAdam,
  'SGDP': SGDP,
  'SGDW': SGDW,
  'Shampoo': Shampoo,
  'SWATS': SWATS,
  'Yogi': Yogi,
  'AdaBound': AdaBound,
  'AdaMod': AdaMod,
  'AdamP': AdamP
}

results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'mae_valid','time'])
i=0
for _, (arch, k) in enumerate(archs):
    print(arch)
    learn = TSForecaster(X, y, splits=splits, path='models', tfms=tfms, batch_tfms=TSStandardize(), bs=128, arch=arch, metrics=[mae], cbs=[ShowGraph(),PredictionDynamics(alpha=.5, size=75)],device=default_device())
    #model = create_model(arch, dls=dls, **k)
    print(learn.__class__.__name__)
    #learn = Learner(dls, model,metrics=[mae,r2_score], cbs=[ShowGraph(),PredictionDynamics(alpha=.5, size=75)]) # dataset , modelo, métricas avaliadas, gráficos de perda e dispersão
    lr = learn.lr_find() # learning rate find
    start = time.time()
    learn.fit_one_cycle(400, lr_max=lr.valley)
    elapsed = time.time() - start
    vals = learn.recorder.values[-1]
    results.loc[i] = [arch.__name__, k, count_parameters(learn.model), vals[0], vals[1], vals[2], int(elapsed)]
    results.sort_values(by=['mae_valid'], ascending=False, kind='stable', ignore_index=True, inplace=True)
    clear_output()
    display(results)
    i+=1

best_architecture = results.loc[len(results) - 1, 'arch']

# def objective(trial):
#     # Categorical parameter
#     optimizer_name = trial.suggest_categorical("optimizer", list(otimizadores.keys()))

#     # Integer parameter
#     optimizer = otimizadores[optimizer_name]

#     encoder_dropout = trial.suggest_float('encoder_dropout', 0.0, 0.5)

#     fc_dropout = trial.suggest_float('fc_dropout', 0.0, 0.5)

#     abs_pos_encode = trial.suggest_categorical('abs_pos_encode', ['tAPE', 'sin', 'learned', None])

#     rel_pos_encode = trial.suggest_categorical('rel_pos_encode', ['eRPE', 'vector', None])

#     arch_config = {
#         'encoder_dropout': encoder_dropout,
#         'fc_dropout': fc_dropout,
#         'abs_pos_encode': abs_pos_encode,
#         'rel_pos_encode': rel_pos_encode
#     }
#     learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True)  # search through all float values between 0.0 and 0.5 in log increment steps

#     hl_delta = trial.suggest_float("delta", 0.1, 2)

#     arch = best_architecture

#     learn = TSForecaster(X, y.values, splits=splits, path='models', tfms=tfms,
#                          batch_tfms=TSStandardize(), bs=128, arch=f"{arch.__name__}",
#                          arch_config= arch_config, metrics= [mae,r2_score],
#                          #cbs=[ShowGraph(),PredictionDynamics(alpha=.5, size=75)],
#                          opt_func=wrap_optimizer(optimizer),loss_func=HuberLoss('mean',hl_delta))

#     with ContextManagers([learn.no_logging(), learn.no_bar()]):
#         for epoch in range(400):
#             learn.fit_one_cycle(1, lr_max=learning_rate)

#             # Report intermediate value
#             intermediate_value = learn.recorder.values[-1][-2]
#             trial.report(intermediate_value, epoch)

#             # Check if trial should be pruned
#             if trial.should_prune():
#                 raise optuna.TrialPruned()

#     return intermediate_value



# study = run_optuna_study(objective,sampler= optuna.samplers.TPESampler(), pruner=optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=20, interval_steps=10), patience=3),n_trials=100,gc_after_trial=True,direction="minimize",show_plots=False)

# learning_rate = study.best_trial.params['learning_rate']
# optimizer = otimizadores[study.best_trial.params['optimizer']]
# delta = study.best_trial.params['delta']
# params_without_lr = {key: value for key, value in study.best_trial.params.items() if (key != 'learning_rate') and (key != 'optimizer') and (key != 'delta')}

# learn = TSForecaster(X, y.values, splits=splits, path='models', tfms=tfms,
#                          batch_tfms=TSStandardize(), bs=128, arch=f"{ConvTranPlus.__name__}",
#                          arch_config= params_without_lr, metrics= [mae,r2_score],
#                          cbs=[ShowGraph(),PredictionDynamics(alpha=.5, size=75)],
#                          opt_func=wrap_optimizer(optimizer),loss_func=HuberLoss('mean',delta))
# learn.fit_one_cycle(400, lr_max=learning_rate)

# raw_preds, target, _ = learn.get_X_preds(X[splits[1]], y.values[splits[1]])
# mae_test = mean_absolute_error(y_pred=raw_preds,y_true=target)
# r2_test = r2_score(y_pred=raw_preds,y_true=target)
# print(r2_test)
# print(mae_test)
# display(np.corrcoef(raw_preds,target))